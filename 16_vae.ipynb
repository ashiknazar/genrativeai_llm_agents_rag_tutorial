{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Variational Autoencoder\n",
        "- A Variational Autoencoder (VAE) is a type of generative model‚Äîthat is, it learns to generate new data that looks like the training data."
      ],
      "metadata": {
        "id": "uZ6vathi4-Lu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder\n",
        "- Instead of mapping the input to a single point (like a regular autoencoder), the encoder maps an input to a distribution over a latent space.\n",
        "- For each input, it outputs:\n",
        "  - A mean vector Œº and\n",
        "  - A log-variance vector log(œÉ¬≤)\n"
      ],
      "metadata": {
        "id": "V9BXxDvP5umR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Latent space sampling\n",
        "- From the distribution(Œº,œÉ¬≤) we sample a latent vector z.\n",
        "- Reparameterization trick\n",
        "  - to allow backpropagation , we sample like this\n",
        "    - z=Œº+œÉ * Œµ\n",
        "    - where Œµ ~ N(0, I)"
      ],
      "metadata": {
        "id": "h1JETo6f7C-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Decoder\n",
        "- The decoder takes z and tries to reconstruct the original input.\n",
        "\n",
        "- It maps z back to the data space (image, text, etc.)."
      ],
      "metadata": {
        "id": "mREN0H-n7S-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective Function (Loss)\n",
        "The VAE‚Äôs loss has two parts:\n",
        "- Reconstruction Loss:\n",
        "  - Measures how well the decoded output matches the input.\n",
        "  - Typically Mean Squared Error (MSE) or Binary Cross Entropy.\n",
        "\n",
        "- KL Divergence Loss:\n",
        "  - A regularization term that pushes the learned distribution (from the encoder) closer to a standard normal distribution N(0, I).\n",
        "  - This makes sure the latent space is smooth and allows meaningful interpolation.\n",
        "  - L=Reconstruction¬†Loss+KL¬†Divergence\n"
      ],
      "metadata": {
        "id": "uAO_B3gr7qHn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gH50_s_38RnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KL Divergence"
      ],
      "metadata": {
        "id": "8Tq__vH08qBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Kullback‚ÄìLeibler Divergence\n",
        "- KL Divergence is a measure of how one probability distribution Q(x) differs from a second, **reference distribution** P(x).\n",
        "- How much information is lost when we approximate P(x) using Q(x)?‚Äù\n",
        "- **Q(z|x)** is the learned (approximate) posterior from the encoder.\n",
        "\n"
      ],
      "metadata": {
        "id": "t390B9Ig8wTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-  the posterior refers to the probability of some unknown variable (here, the latent variable z) after observing data x.\n",
        "- Given this data x, what is the likely distribution of the hidden variable z?\n",
        "  p(z‚à£x)=posterior\n",
        "  "
      ],
      "metadata": {
        "id": "RWCgOwpn8w5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- x = your data (like an image)\n",
        "- z = latent variable (compressed, hidden representation)\n",
        "- p(z) = prior distribution (often chosen to be a standard normal)\n",
        "- p(x|z) = likelihood (decoder: how likely x is given z)\n",
        "- p(z|x) = posterior (how likely z is given the observed x)"
      ],
      "metadata": {
        "id": "WdaM3yz7BXC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùå Why can't we use the real posterior?\n"
      ],
      "metadata": {
        "id": "IJ5ogaVzBts9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The true posterior p(z‚à£x) is usually intractable because it involves calculating:\n",
        "   $p(z|x)=p(x|z)p(z) /  p(x)$\n",
        "   \n"
      ],
      "metadata": {
        "id": "du0jt3kPByLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "and $ p(x)=‚à´p(x‚à£z)p(z)dz $ is hard to compute (because the integral is often impossible to solve exactly)."
      ],
      "metadata": {
        "id": "YJb34D2sCU8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- So what do we do?We approximate the posterior\n",
        "- We introduce a learned function called the encoder, which gives\n",
        " $Q(z‚à£x)$\n",
        "- This is a simpler, approximate distribution (like a Gaussian) that tries to be close to the true posterior\n",
        "p(z‚à£x)\n",
        "- The training goal is to make Q(z‚à£x) as close as possible to p(z‚à£x), typically by minimizing their KL divergence."
      ],
      "metadata": {
        "id": "THUQkjc3CiUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian regression"
      ],
      "metadata": {
        "id": "7CKdyjSNHw5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "NxObUWMBF3wJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$p(z‚à£x)= p(x‚à£z)‚ãÖp(z) / p(x)$\n",
        "‚Äã\n",
        "\n"
      ],
      "metadata": {
        "id": "95KakY3CF6wZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- p(x‚à£z): likelihood (decoder)\n",
        "- p(z): prior\n",
        "- p(x)=‚à´p(x‚à£z)p(z)dz: marginal likelihood (evidence)\n",
        "- The problem is this denominator,\n",
        "p(x), which requires integrating over all possible values of z.\n",
        "- **In Bayesian regression, sometimes we avoid this integration or simplify** it because:\n",
        "   - We assume conjugate priors ‚Üí the posterior has a closed form.\n",
        "   - We use sampling methods like MCMC to approximate it.\n",
        "   - Or we just compute MAP (maximum a posteriori) instead of full p(z‚à£x).\n",
        "   - **So even in Bayesian regression, we often don‚Äôt explicitly compute the full posterior ‚Äî we approximate or optimize over it.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Ep5tXlRpGRGS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "sJwsy8HfHtOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üß† KL Divergence in Variational Autoencoders (VAEs)\n",
        "\n",
        "**KL Divergence** (Kullback‚ÄìLeibler Divergence) measures how one probability distribution diverges from another expected probability distribution. In VAEs, it regularizes the latent space by comparing the learned latent distribution $Q(z|x)$ with a prior $P(z)$.\n",
        "\n",
        "---\n",
        "\n",
        "### üìê Definition\n",
        "\n",
        "The KL divergence from distribution $Q(z)$ (approximate posterior) to $P(z)$ (prior) is defined as:\n",
        "\n",
        "$$\n",
        "\\text{KL}(Q || P) = \\int Q(z) \\log \\left( \\frac{Q(z)}{P(z)} \\right) \\, dz\n",
        "$$\n",
        "\n",
        "This measures how much information is lost when using $Q$ to approximate $P$. KL divergence is always non-negative and equals zero only when $Q = P$.\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ KL Divergence Between Two Gaussians\n",
        "\n",
        "In VAEs:\n",
        "\n",
        "- The encoder outputs a Gaussian: $Q(z|x) = \\mathcal{N}(\\mu, \\sigma^2)$\n",
        "- The prior is a standard Gaussian: $P(z) = \\mathcal{N}(0, 1)$\n",
        "\n",
        "The KL divergence between these two distributions has a closed-form solution:\n",
        "\n",
        "$$\n",
        "\\text{KL}\\left( \\mathcal{N}(\\mu, \\sigma^2) \\, || \\, \\mathcal{N}(0, 1) \\right) = \\frac{1}{2} \\sum_{i=1}^{d} \\left( \\mu_i^2 + \\sigma_i^2 - \\log \\sigma_i^2 - 1 \\right)\n",
        "$$\n",
        "\n",
        "Here:\n",
        "- $\\mu_i$ is the mean of the $i^{\\text{th}}$ latent dimension  \n",
        "- $\\sigma_i^2$ is the variance of the $i^{\\text{th}}$ latent dimension  \n",
        "- $d$ is the dimensionality of the latent space\n",
        "\n",
        "---\n",
        "\n",
        "### üéØ Why This Matters in VAEs\n",
        "\n",
        "- Encourages the encoder to produce a **latent distribution** close to a **standard normal**: $z \\sim \\mathcal{N}(0, I)$  \n",
        "- Ensures the latent space is **continuous and smooth**  \n",
        "- Allows us to **sample** and **interpolate** meaningfully in the latent space  \n",
        "- Prevents overfitting by regularizing the encoder\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ In practice, this KL term is added to the **VAE loss function**, along with a reconstruction loss, to form the total objective:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\text{VAE}} = \\mathbb{E}_{Q(z|x)} \\left[ \\log P(x|z) \\right] - \\text{KL}(Q(z|x) || P(z))\n",
        "$$\n",
        "\n",
        "This balances reconstruction accuracy and latent space regularization.\n"
      ],
      "metadata": {
        "id": "BWA65pA7IkS0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "___"
      ],
      "metadata": {
        "id": "up3OdwMLMBbt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ 1. Overview of VAE Architecture\n",
        "\n",
        "> A VAE consists of:\n",
        "- **Encoder**: Maps input $x$ to latent distribution parameters $\\mu$ and $\\log\\sigma^2$\n",
        "- **Reparameterization trick**: Samples latent variable $z$ from the latent distribution using $z = \\mu + \\sigma \\cdot \\epsilon$\n",
        "- **Decoder**: Reconstructs $x$ from $z$\n",
        "- **Loss**: Combines reconstruction loss + KL divergence\n",
        "\n"
      ],
      "metadata": {
        "id": "MuucZ8jVMFR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Sampling latent variable** $z$ just means:\n",
        "  -  Randomly generating a point from that Gaussian distribution, using:\n",
        "  - z=Œº+œÉ‚ãÖœµ\n",
        "  - where $\\epsilon \\sim \\mathcal{N}(0, I)$ ‚Äî random noise from a standard normal distribution (mean 0, std 1).\n"
      ],
      "metadata": {
        "id": "lAa-GeNSOP1D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ü§Ø Why Reparameterize?\n",
        "\n",
        "*   We need to backpropagate gradients during training.\n",
        "*   But sampling from a distribution (e.g., torch.randn()) is not differentiable.\n",
        "* To fix that, we use the reparameterization trick:\n",
        "   - Instead of directly sampling $z \\sim \\mathcal{N}(\\mu, \\sigma^2)$ (not differentiable),\n",
        "\n",
        "   - We sample $\\epsilon \\sim \\mathcal{N}(0, 1)$ (which is fixed and known),\n",
        "\n",
        "   - And compute: $z = \\mu + \\sigma \\cdot \\epsilon$\n",
        "‚úÖ This formula is differentiable w.r.t. $\\mu$ and $\\sigma$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zDlgXf34OpqC"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qEr-IgzpHq28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîç Problem: Why Not Just Sample Normally?\n",
        "> - In a standard autoencoder, you encode input $x$ to a latent vector $z$ (just a point).\n",
        "  - In a VAE, you want to model uncertainty, so instead of mapping $x \\rightarrow z$,\n",
        "you map $x \\rightarrow \\mu, \\log\\sigma^2$ and assume:\n",
        "   - $z‚àºN(Œº,œÉ_{2})$\n",
        "- That is, $z$ is drawn from a Gaussian distribution for each input.\n",
        "\n",
        "\n",
        "- ‚ùó But here's the issue: sampling a random variable breaks backpropagation because it's not differentiable.\n"
      ],
      "metadata": {
        "id": "3zxX2dr7P2pC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- $\n",
        "\\nabla_\\theta \\, \\mathbb{E}_{z \\sim q_\\theta(z|x)} \\left[ \\log p_\\phi(x|z) \\right]\n",
        "$\n",
        "- But we can‚Äôt differentiate through the stochastic node where $z$ is sampled.\n",
        "- üí° Solution: Reparameterization Trick\n",
        "\n",
        "  - We make $z$ deterministic by moving the randomness outside the network.\n",
        "\n",
        "Instead of:$$\n",
        "z‚àºN(Œº,œÉ^2)$$\n",
        "we rewrite it as:\n",
        "\n",
        "\n",
        "$$ z=Œº+œÉ‚ãÖœµ $$ where¬†$$œµ‚àºN(0,1)$$\n",
        "This works because:\n",
        "\n",
        "- $\\mu$ and $\\sigma$ are outputs from the network (dependent on input $x$)\n",
        "\n",
        "- $\\epsilon$ is just noise (independent of the network)\n",
        "\n",
        "- Now, $z$ is a deterministic function of $x$ and $\\epsilon$\n",
        "\n",
        "‚úÖ This form is differentiable with respect to $\\mu$ and $\\sigma$, so we can use backpropagation!\n"
      ],
      "metadata": {
        "id": "9XexxLt7StsS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LVPeJljoQaHX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}