{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8895a4bc",
   "metadata": {},
   "source": [
    "# ğŸŒ€ Week 3: Autoencoders and Variational Autoencoders (VAEs)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” What are Autoencoders?\n",
    "\n",
    "Autoencoders are a type of neural network used to **learn efficient representations (encodings)** of input data, typically for **dimensionality reduction** or **feature learning**.\n",
    "\n",
    "They consist of two parts:\n",
    "- **Encoder**: Compresses input into a latent space representation\n",
    "- **Decoder**: Reconstructs the original input from the latent representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8beb73a0",
   "metadata": {},
   "source": [
    "Input â†’ [Encoder] â†’ Latent Vector (z) â†’ [Decoder] â†’ Reconstructed Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3035e5",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Architecture of a Basic Autoencoder\n",
    "\n",
    "- **Input Layer**: Original data (e.g., image pixels)\n",
    "- **Encoder Network**: Series of layers that reduce dimensionality\n",
    "- **Latent Space**: Encoded, compressed representation of input\n",
    "- **Decoder Network**: Reconstructs data from the latent vector\n",
    "- **Output Layer**: Same shape as input\n",
    "\n",
    "> ğŸ§ª Loss Function: Usually **Mean Squared Error (MSE)**  \n",
    "> \\( \\mathcal{L} = \\| x - \\hat{x} \\|^2 \\)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”„ Applications of Autoencoders\n",
    "\n",
    "- ğŸ”§ **Noise Removal (Denoising Autoencoders)**\n",
    "- ğŸ§¬ **Dimensionality Reduction**\n",
    "- ğŸ” **Anomaly Detection**\n",
    "- ğŸ§  **Feature Extraction**\n",
    "- ğŸ¨ **Image Colorization, Inpainting**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§¬ Variational Autoencoders (VAEs)\n",
    "\n",
    "VAEs are **probabilistic generative models** that extend autoencoders by learning a **distribution over the latent space**, not just point estimates.\n",
    "\n",
    "### ğŸ§  Key Idea:\n",
    "Instead of learning a single point \\( z \\), VAEs learn:\n",
    "- \\( \\mu(x) \\): mean of latent distribution\n",
    "- \\( \\sigma(x) \\): standard deviation\n",
    "\n",
    "> Then sample from \\( \\mathcal{N}(\\mu, \\sigma^2) \\) to get \\( z \\)\n",
    "\n",
    "### âœï¸ Why This Matters?\n",
    "- VAEs allow for **smooth interpolation** and **new data generation**\n",
    "- You can **sample** from the latent space to generate new examples\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ VAE Loss Function\n",
    "\n",
    "The VAE combines two components:\n",
    "\n",
    "1. **Reconstruction Loss** (e.g., MSE or Binary Cross-Entropy):  \n",
    "   Ensures output is close to the original\n",
    "\n",
    "2. **KL Divergence Loss**:  \n",
    "   Ensures the learned latent distribution is close to a prior (e.g., Normal)\n",
    "\n",
    "> **Total Loss:**  \n",
    "> \\( \\mathcal{L}_{\\text{VAE}} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - D_{KL}(q(z|x) \\| p(z)) \\)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Visual Intuition\n",
    "\n",
    "- A **standard autoencoder** maps inputs to *specific points* in the latent space.\n",
    "- A **VAE** maps inputs to *distributions* in the latent space â€” enabling **controlled sampling** and **diversity in generation**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¬ Use Cases of VAEs\n",
    "\n",
    "- ğŸ”¬ Generating images and synthetic data\n",
    "- ğŸ§¬ Protein design and drug discovery\n",
    "- ğŸ¨ Artistic style transfer\n",
    "- ğŸ§  Latent space visualization and exploration\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“š Suggested Resources\n",
    "\n",
    "- Blog: [â€œA Beginnerâ€™s Guide to VAEsâ€ by Carl Doersch](https://arxiv.org/abs/1606.05908)\n",
    "- [Google Colab: VAE with Keras/TensorFlow](https://colab.research.google.com/github/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Variational_Autoencoder.ipynb)\n",
    "- Book: *Deep Learning* by Ian Goodfellow â€“ Chapters on autoencoders\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’» Optional Code Practice\n",
    "\n",
    "```python\n",
    "# Simple Autoencoder with Keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "# Encoder\n",
    "input_img = Input(shape=(784,))\n",
    "encoded = Dense(64, activation='relu')(input_img)\n",
    "\n",
    "# Decoder\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder Model\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71c5d83",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
