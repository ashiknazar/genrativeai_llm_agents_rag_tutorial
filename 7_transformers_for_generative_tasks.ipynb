{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc79e38",
   "metadata": {},
   "source": [
    "# ðŸ¤– Week 7: Transformers for Generative Tasks\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ What Are Transformers?\n",
    "\n",
    "Transformers are **deep learning architectures** based on **self-attention mechanisms**, first introduced in the paper  \n",
    "> â€œAttention Is All You Needâ€ (Vaswani et al., 2017).  \n",
    "\n",
    "They have revolutionized **sequence modeling** by enabling **parallel processing**, better **long-range dependency modeling**, and **scalability**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Core Concepts Recap\n",
    "\n",
    "- **Self-Attention**: Allows the model to weigh the importance of each part of the input sequence dynamically.\n",
    "- **Positional Encoding**: Since Transformers lack recurrence, they use position encodings to capture sequence order.\n",
    "- **Encoder-Decoder Architecture**:\n",
    "  - Encoder: Processes the input\n",
    "  - Decoder: Generates output sequence (used in tasks like translation, summarization)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸª„ Why Transformers for Generation?\n",
    "\n",
    "Transformers excel in generative tasks due to:\n",
    "- **Scalability to large data and models**\n",
    "- **Effective modeling of sequential and contextual information**\n",
    "- **Generalization across modalities (text, image, audio)**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ Text Generation\n",
    "\n",
    "| Model          | Description |\n",
    "|----------------|-------------|\n",
    "| **GPT (OpenAI)**   | Autoregressive decoder-only transformer trained to predict next token |\n",
    "| **BERT (Google)**  | Encoder-only model, good for understanding but not generation |\n",
    "| **T5 (Text-to-Text Transfer Transformer)** | Unified text-to-text framework, supports summarization, Q&A, translation |\n",
    "| **BART**         | Encoder-decoder trained with denoising objectives, good for summarization and translation |\n",
    "| **LLaMA, PaLM, Mistral** | Large-scale open and closed-source language models for versatile generation |\n",
    "\n",
    "> ðŸ” These models predict output **one token at a time**, conditioning on previously generated tokens (autoregressive).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Image Generation with Transformers\n",
    "\n",
    "Transformers have also been adapted for **image synthesis**:\n",
    "\n",
    "| Model             | Highlights |\n",
    "|------------------|------------|\n",
    "| **DALLÂ·E**       | Transformer-based text-to-image generation using VQ-VAE tokenized images |\n",
    "| **Imagen (Google)** | Text-to-image diffusion guided by language models |\n",
    "| **MaskGIT, Parti** | Image generation using discrete tokens and transformer-based decoding |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”Š Audio & Multimodal Generation\n",
    "\n",
    "Transformers are also used in:\n",
    "- **Speech synthesis** (e.g., FastSpeech, SpeechT5)\n",
    "- **Music generation** (e.g., Music Transformer, Jukebox)\n",
    "- **Multimodal generation** (e.g., CLIP, Flamingo, Gato)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Key Applications\n",
    "\n",
    "| Domain             | Use Case Examples                              |\n",
    "|--------------------|------------------------------------------------|\n",
    "| ðŸ§¾ Text             | Story generation, summarization, translation   |\n",
    "| ðŸŽ¨ Vision           | Text-to-image, inpainting, style transfer      |\n",
    "| ðŸ—£ï¸ Audio            | Text-to-speech, music composition              |\n",
    "| ðŸŒ Multimodal       | Image captioning, video narration              |\n",
    "| ðŸ§ª Code Generation  | GitHub Copilot, ChatGPT coding                 |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Notable Papers & Resources\n",
    "\n",
    "- [Attention is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "- [GPT-3: Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n",
    "- [T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)\n",
    "- [DALLÂ·E](https://openai.com/dall-e)\n",
    "- [Imagen](https://imagen.research.google/)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¬ Discussion Prompt\n",
    "\n",
    "> How do decoder-only transformers differ from encoder-decoder transformers in generative tasks?  \n",
    "> What challenges arise when using transformers for high-resolution image or video generation?\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- Transformers have become the **backbone of generative AI** across modalities.\n",
    "- Decoder-only (e.g., GPT) and encoder-decoder (e.g., T5, BART) serve different purposes.\n",
    "- Combined with tokenization and pretraining strategies, they enable powerful generation tasks in **text, vision, audio, and beyond**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996255de",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
