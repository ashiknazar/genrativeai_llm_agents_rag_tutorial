{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bee33176",
   "metadata": {},
   "source": [
    "## üîç Introduction to Autoencoders\n",
    "\n",
    "Autoencoders are a type of artificial neural network used to learn efficient representations of data. They are primarily used for:\n",
    "\n",
    "- Dimensionality Reduction\n",
    "- Feature Learning\n",
    "- Denoising\n",
    "- Anomaly Detection\n",
    "\n",
    "### üß† Structure\n",
    "\n",
    "An autoencoder consists of two parts:\n",
    "\n",
    "- **Encoder**: Compresses input data into a latent (bottleneck) representation. $ z=f(x)$ , x-input,z-encoded representation\n",
    "- **Decoder**: Reconstructs the input from the latent representation.\n",
    "\n",
    "The network is trained to minimize the reconstruction error:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\| x - \\hat{x} \\|^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x$ = original input\n",
    "- $\\hat{x}$ = reconstructed output\n",
    "### Why Use Autoencoders?\n",
    "- **Dimensionality Reduction**: Similar to PCA but can capture nonlinear relationships.\n",
    "\n",
    "- **Denoising**: Remove noise from input data.\n",
    "\n",
    "- **Anomaly Detection**: Autoencoders trained on \"normal\" data will have high reconstruction errors on anomalies.\n",
    "\n",
    "- **Feature Extraction**: Use the encoder‚Äôs output as input for other ML models.\n",
    "\n",
    "### üîß Variants of Autoencoders\n",
    "\n",
    "| Type | Description |\n",
    "|------|-------------|\n",
    "| Vanilla Autoencoder | Basic structure with one hidden layer |\n",
    "| Denoising Autoencoder | Learns to reconstruct clean inputs from noisy ones |\n",
    "| Sparse Autoencoder | Adds a sparsity constraint to encourage feature selection |\n",
    "| Variational Autoencoder (VAE) | Learns a distribution over the latent space(used in GAN) |\n",
    "| Convolutional Autoencoder | Uses CNNs for image data |\n",
    "| Sequence Autoencoder | Used for sequence data like text or time series |\n",
    "\n",
    "### üì¶ Use Cases\n",
    "\n",
    "- Image compression and denoising\n",
    "- Pretraining layers for deep networks\n",
    "- Anomaly detection in time-series and tabular data\n",
    "- Latent space representation for clustering\n",
    "\n",
    "### üöÄ Sample PyTorch Autoencoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e19312",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 784),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9ae3c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc44d5e9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6065c7ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74ddb188",
   "metadata": {},
   "source": [
    "####\n",
    "- An autoencoder is a type of neural network that learns to compress and reconstruct data. When it's trained on normal data only, it learns to reconstruct that kind of data very accurately.\n",
    "\n",
    "So, when you give it anomalous (unusual or abnormal) data, the autoencoder fails to reconstruct it well, resulting in a high reconstruction error. That error can be used to detect anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a6025",
   "metadata": {},
   "source": [
    "#### Denoising Autoencoder\n",
    "- a Denoising Autoencoder (DAE), the original input is not considered noisy ‚Äî instead, you intentionally add noise to the input during training so the model learns to remove the noise and recover the clean, original data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d07968f",
   "metadata": {},
   "source": [
    "#### Adding sparsity to encourage feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5299c1d",
   "metadata": {},
   "source": [
    "- Sparsity means that most neurons (especially in the hidden layer) are inactive (i.e., output values near zero) for a given input.\n",
    "- In a Sparse Autoencoder, this behavior is intentionally encouraged ‚Äî not all hidden units should be \"on\" for every input.\n",
    "- Because only a small subset of neurons are active for a given input, each neuron ends up specializing in detecting specific features.\n",
    "   - This leads to feature selection because:\n",
    "       -  The network learns to activate only those neurons that represent the most salient or distinguishing features of the input.\n",
    "       -  Irrelevant features are suppressed (i.e., not used by any neuron), similar to how L1 regularization works in models like Lasso Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33795b84",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
