{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9f4212",
   "metadata": {},
   "source": [
    "# ðŸ“Š Week 2: Probability and Machine Learning Foundations\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Objective\n",
    "\n",
    "To understand the **mathematical foundations** necessary for working with generative models. This includes essential **probability theory**, **statistics**, and **core machine learning concepts** that underpin generative AI algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ² Probability & Statistics Refresher\n",
    "\n",
    "### ðŸ”¢ Key Concepts\n",
    "\n",
    "- **Random Variables**: Variables whose possible values are outcomes of a random phenomenon.\n",
    "- **Probability Distributions**: Describe how probabilities are distributed over values (e.g., Normal, Bernoulli, Binomial, Multinomial).\n",
    "- **Mean (Î¼)** and **Variance (ÏƒÂ²)**: Central tendency and spread of distributions.\n",
    "- **Expectation**: Average value of a random variable:  \n",
    "  \\( \\mathbb{E}[X] = \\sum x \\cdot P(x) \\)\n",
    "- **Conditional Probability**:  \n",
    "  \\( P(A|B) = \\frac{P(A \\cap B)}{P(B)} \\)\n",
    "- **Bayesâ€™ Theorem**:  \n",
    "  \\( P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} \\)\n",
    "- **KL Divergence**: A measure of how one probability distribution diverges from another:  \n",
    "  \\( D_{KL}(P || Q) = \\sum P(x) \\log \\frac{P(x)}{Q(x)} \\)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Machine Learning Foundations\n",
    "\n",
    "### ðŸ§® Supervised vs. Unsupervised Learning\n",
    "\n",
    "|                      | **Supervised Learning**         | **Unsupervised Learning**         |\n",
    "|----------------------|----------------------------------|------------------------------------|\n",
    "| **Goal**             | Predict labels                  | Discover hidden patterns           |\n",
    "| **Input**            | Features + labels               | Features only                      |\n",
    "| **Examples**         | Regression, Classification      | Clustering, Dimensionality Reduction |\n",
    "\n",
    "### ðŸ“ˆ Loss Functions\n",
    "- **MSE (Mean Squared Error)** â€“ Regression tasks\n",
    "- **Cross-Entropy Loss** â€“ Classification tasks\n",
    "- **Binary Cross-Entropy** â€“ Binary classification\n",
    "- **KL Divergence** â€“ Used in VAEs, distributions\n",
    "\n",
    "### ðŸ”§ Optimization Techniques\n",
    "- **Gradient Descent**\n",
    "- **Backpropagation**\n",
    "- **Learning Rate Scheduling**\n",
    "- **Stochastic Gradient Descent (SGD)**\n",
    "- **Adam Optimizer**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ¤– Why These Foundations Matter for Generative AI\n",
    "\n",
    "| Concept                   | Importance in Generative Models                                 |\n",
    "|---------------------------|------------------------------------------------------------------|\n",
    "| **Probability**           | Generative models model **data distributions**                  |\n",
    "| **Bayesâ€™ Theorem**        | Basis for **VAEs**, probabilistic inference                     |\n",
    "| **KL Divergence**         | Used in training VAEs and comparing model outputs                |\n",
    "| **Latent Variables**      | Represent **hidden features** in data (e.g., z in VAE)           |\n",
    "| **Loss Functions**        | Crucial for training models like **GANs, VAEs, Diffusion**       |\n",
    "| **Optimization**          | Used in training **deep neural networks** underlying GenAI      |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Optional Practice / Assignments\n",
    "\n",
    "1. Compute:\n",
    "   - Mean, variance, and entropy of a small dataset\n",
    "   - KL Divergence between two simple distributions\n",
    "2. Classify points using:\n",
    "   - Logistic Regression\n",
    "   - Decision Tree\n",
    "3. Visualize:\n",
    "   - Probability distributions using matplotlib/seaborn\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Suggested Resources\n",
    "\n",
    "- [StatQuest YouTube](https://www.youtube.com/user/joshstarmer) â€“ Excellent visual explanations\n",
    "- *Pattern Recognition and Machine Learning* by Christopher Bishop\n",
    "- *Probabilistic Machine Learning* by Kevin Murphy\n",
    "- [Khan Academy â€“ Statistics & Probability](https://www.khanacademy.org/math/statistics-probability)\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ§  **Pro Tip for Students:** A strong foundation in probability and optimization will help you deeply understand how generative models *learn* to generate content.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b26849",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
