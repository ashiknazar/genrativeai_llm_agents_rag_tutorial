{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd842959",
   "metadata": {},
   "source": [
    "# 🎥 Week 9: Multimodal Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 What is Multimodal Generative AI?\n",
    "\n",
    "**Multimodal Generative AI** refers to models that can understand and generate data across multiple modalities such as:\n",
    "\n",
    "- 📝 Text\n",
    "- 🖼️ Images\n",
    "- 🔊 Audio\n",
    "- 🎞️ Video\n",
    "- 🧑‍🤝‍🧑 Human interactions (e.g., gestures, speech)\n",
    "\n",
    "These models **bridge multiple data types** and enable rich generative experiences — such as generating an image from a prompt or captioning a video.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Why Multimodal?\n",
    "\n",
    "Real-world data is multimodal. Humans naturally **perceive, communicate, and understand through multiple senses**. Generative AI aims to replicate this behavior:\n",
    "\n",
    "| Modality | Task Example                         |\n",
    "|----------|--------------------------------------|\n",
    "| Text + Image | Text-to-image (e.g., DALL·E)       |\n",
    "| Image + Text | Image captioning, VQA             |\n",
    "| Audio + Text | Speech-to-text, voice generation  |\n",
    "| Video + Text | Video narration, scene description|\n",
    "| Code + Text | Generate visual UI from text       |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Key Multimodal Models\n",
    "\n",
    "| Model          | Modality Support | Capabilities                            |\n",
    "|----------------|------------------|-----------------------------------------|\n",
    "| **CLIP**       | Text + Image     | Learns joint embedding space            |\n",
    "| **DALL·E 2**   | Text → Image     | High-resolution image generation        |\n",
    "| **Flamingo**   | Text + Image     | Visual question answering, captioning   |\n",
    "| **Gato**       | Many             | Single model for multiple tasks         |\n",
    "| **BLIP / BLIP-2** | Image + Text  | Captioning, visual Q&A, prompt tuning   |\n",
    "| **Stable Diffusion** | Text → Image | Open-source diffusion for image gen    |\n",
    "| **Make-A-Video (Meta)** | Text → Video | Short video generation              |\n",
    "| **Gemini**     | Multimodal       | Successor to Bard with enhanced fusion  |\n",
    "| **LLaVA**      | Vision + LLM     | Visual + language assistant             |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔄 How It Works\n",
    "\n",
    "Multimodal models either:\n",
    "- **Fuse different modalities** in a shared representation space (e.g., joint embeddings)\n",
    "- **Generate one modality from another** (e.g., image from text using diffusion models)\n",
    "\n",
    "They typically use:\n",
    "- **Cross-attention mechanisms**\n",
    "- **Multimodal encoders & decoders**\n",
    "- **Adapters for modality-specific tokens**\n",
    "\n",
    "---\n",
    "\n",
    "## 🖇️ Common Architectures\n",
    "\n",
    "- **Two-Tower Models**: Independent encoders for each modality, merged later (e.g., CLIP)\n",
    "- **Unified Transformers**: One model processes everything with modality-specific embeddings\n",
    "- **Encoder-Decoder + Diffusion**: Encodes input (text) and generates modality (image/video)\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Applications\n",
    "\n",
    "| Domain           | Use Case                               |\n",
    "|------------------|----------------------------------------|\n",
    "| Creativity       | Text-to-image, music generation         |\n",
    "| Education        | Visual explanations, multimodal tutors |\n",
    "| Accessibility    | Image captioning for the visually impaired |\n",
    "| Healthcare       | Radiology reports from images           |\n",
    "| Social Media     | Auto-captioning, meme generation        |\n",
    "| E-commerce       | Virtual try-on, image-based search      |\n",
    "\n",
    "---\n",
    "\n",
    "## 📚 Notable Resources\n",
    "\n",
    "- [CLIP (OpenAI)](https://openai.com/research/clip)\n",
    "- [DALL·E](https://openai.com/dall-e)\n",
    "- [BLIP-2](https://github.com/salesforce/BLIP)\n",
    "- [Meta's Make-A-Video](https://makeavideo.studio/)\n",
    "- [Google's Gemini](https://deepmind.google/technologies/gemini/)\n",
    "\n",
    "---\n",
    "\n",
    "## 💬 Discussion Prompt\n",
    "\n",
    "> How do you think multimodal models will impact creative industries like design or filmmaking?  \n",
    "> What challenges arise when aligning meaning across different modalities?\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- Multimodal generative AI opens up new frontiers by integrating vision, text, audio, and beyond.\n",
    "- These models enable creative, conversational, and assistive applications.\n",
    "- Key advances include shared embeddings, transformer fusion, and diffusion-based generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ba38b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
