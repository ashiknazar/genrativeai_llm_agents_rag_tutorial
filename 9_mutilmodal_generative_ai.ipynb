{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd842959",
   "metadata": {},
   "source": [
    "# ðŸŽ¥ Week 9: Multimodal Generative AI\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ What is Multimodal Generative AI?\n",
    "\n",
    "**Multimodal Generative AI** refers to models that can understand and generate data across multiple modalities such as:\n",
    "\n",
    "- ðŸ“ Text\n",
    "- ðŸ–¼ï¸ Images\n",
    "- ðŸ”Š Audio\n",
    "- ðŸŽžï¸ Video\n",
    "- ðŸ§‘â€ðŸ¤â€ðŸ§‘ Human interactions (e.g., gestures, speech)\n",
    "\n",
    "These models **bridge multiple data types** and enable rich generative experiences â€” such as generating an image from a prompt or captioning a video.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Why Multimodal?\n",
    "\n",
    "Real-world data is multimodal. Humans naturally **perceive, communicate, and understand through multiple senses**. Generative AI aims to replicate this behavior:\n",
    "\n",
    "| Modality | Task Example                         |\n",
    "|----------|--------------------------------------|\n",
    "| Text + Image | Text-to-image (e.g., DALLÂ·E)       |\n",
    "| Image + Text | Image captioning, VQA             |\n",
    "| Audio + Text | Speech-to-text, voice generation  |\n",
    "| Video + Text | Video narration, scene description|\n",
    "| Code + Text | Generate visual UI from text       |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Key Multimodal Models\n",
    "\n",
    "| Model          | Modality Support | Capabilities                            |\n",
    "|----------------|------------------|-----------------------------------------|\n",
    "| **CLIP**       | Text + Image     | Learns joint embedding space            |\n",
    "| **DALLÂ·E 2**   | Text â†’ Image     | High-resolution image generation        |\n",
    "| **Flamingo**   | Text + Image     | Visual question answering, captioning   |\n",
    "| **Gato**       | Many             | Single model for multiple tasks         |\n",
    "| **BLIP / BLIP-2** | Image + Text  | Captioning, visual Q&A, prompt tuning   |\n",
    "| **Stable Diffusion** | Text â†’ Image | Open-source diffusion for image gen    |\n",
    "| **Make-A-Video (Meta)** | Text â†’ Video | Short video generation              |\n",
    "| **Gemini**     | Multimodal       | Successor to Bard with enhanced fusion  |\n",
    "| **LLaVA**      | Vision + LLM     | Visual + language assistant             |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”„ How It Works\n",
    "\n",
    "Multimodal models either:\n",
    "- **Fuse different modalities** in a shared representation space (e.g., joint embeddings)\n",
    "- **Generate one modality from another** (e.g., image from text using diffusion models)\n",
    "\n",
    "They typically use:\n",
    "- **Cross-attention mechanisms**\n",
    "- **Multimodal encoders & decoders**\n",
    "- **Adapters for modality-specific tokens**\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ–‡ï¸ Common Architectures\n",
    "\n",
    "- **Two-Tower Models**: Independent encoders for each modality, merged later (e.g., CLIP)\n",
    "- **Unified Transformers**: One model processes everything with modality-specific embeddings\n",
    "- **Encoder-Decoder + Diffusion**: Encodes input (text) and generates modality (image/video)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“ˆ Applications\n",
    "\n",
    "| Domain           | Use Case                               |\n",
    "|------------------|----------------------------------------|\n",
    "| Creativity       | Text-to-image, music generation         |\n",
    "| Education        | Visual explanations, multimodal tutors |\n",
    "| Accessibility    | Image captioning for the visually impaired |\n",
    "| Healthcare       | Radiology reports from images           |\n",
    "| Social Media     | Auto-captioning, meme generation        |\n",
    "| E-commerce       | Virtual try-on, image-based search      |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Notable Resources\n",
    "\n",
    "- [CLIP (OpenAI)](https://openai.com/research/clip)\n",
    "- [DALLÂ·E](https://openai.com/dall-e)\n",
    "- [BLIP-2](https://github.com/salesforce/BLIP)\n",
    "- [Meta's Make-A-Video](https://makeavideo.studio/)\n",
    "- [Google's Gemini](https://deepmind.google/technologies/gemini/)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¬ Discussion Prompt\n",
    "\n",
    "> How do you think multimodal models will impact creative industries like design or filmmaking?  \n",
    "> What challenges arise when aligning meaning across different modalities?\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- Multimodal generative AI opens up new frontiers by integrating vision, text, audio, and beyond.\n",
    "- These models enable creative, conversational, and assistive applications.\n",
    "- Key advances include shared embeddings, transformer fusion, and diffusion-based generation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1ba38b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
