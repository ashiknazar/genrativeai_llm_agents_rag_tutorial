{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "988e749c",
   "metadata": {},
   "source": [
    "# ðŸ§  Week 8: Large Language Models (LLMs)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŒ What Are LLMs?\n",
    "\n",
    "**Large Language Models (LLMs)** are deep learning models trained on massive text corpora to understand and generate human-like text.  \n",
    "They are typically **transformer-based**, containing **hundreds of millions to trillions of parameters**.\n",
    "\n",
    "LLMs are capable of:\n",
    "- Text generation\n",
    "- Summarization\n",
    "- Translation\n",
    "- Code completion\n",
    "- Reasoning and question answering\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¨ Key Characteristics\n",
    "\n",
    "| Characteristic        | Description                                                                 |\n",
    "|------------------------|-----------------------------------------------------------------------------|\n",
    "| **Transformer Backbone** | Uses decoder-only or encoder-decoder architectures                      |\n",
    "| **Pretraining**        | Trained on large datasets using self-supervised learning (e.g., predicting next token) |\n",
    "| **Scaling Laws**       | Performance improves with model size, dataset size, and compute           |\n",
    "| **Zero-shot / Few-shot** | Can perform tasks with little to no fine-tuning                         |\n",
    "| **In-context Learning** | Learns patterns during inference based on input prompt                   |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ† Prominent LLMs\n",
    "\n",
    "| Model         | Organization | Parameters | Highlights                          |\n",
    "|---------------|--------------|------------|-------------------------------------|\n",
    "| **GPT-3**     | OpenAI       | 175B       | Few-shot capabilities, versatile    |\n",
    "| **GPT-4**     | OpenAI       | ~1T?       | Multimodal support, strong reasoning|\n",
    "| **LLaMA 2**   | Meta         | 7Bâ€“70B     | Open-weight model, efficient        |\n",
    "| **PaLM**      | Google       | 540B       | Generalist with multilingual support|\n",
    "| **Claude**    | Anthropic    | ~100B+     | Constitutional AI safety methods    |\n",
    "| **Mistral**   | Mistral AI   | 7Bâ€“12.9B   | Compact, efficient, open source     |\n",
    "| **Gemini**    | Google DeepMind | ??     | Multimodal successor to Bard        |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ› ï¸ How They Work\n",
    "\n",
    "1. **Pretraining Phase**:\n",
    "   - Train on massive datasets (e.g., books, web data) using unsupervised objectives like next-token prediction.\n",
    "\n",
    "2. **Fine-tuning Phase**:\n",
    "   - Refine on domain-specific or task-specific datasets.\n",
    "   - RLHF (Reinforcement Learning from Human Feedback) enhances alignment.\n",
    "\n",
    "3. **Inference / Prompt Engineering**:\n",
    "   - Craft prompts to elicit desired behaviors (e.g., chain-of-thought, role-playing, formatting).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ’¡ Capabilities\n",
    "\n",
    "- Natural language conversation (e.g., ChatGPT)\n",
    "- Code generation (e.g., GitHub Copilot)\n",
    "- Language translation\n",
    "- Story and poem writing\n",
    "- Knowledge retrieval and summarization\n",
    "- Logical reasoning and mathematics\n",
    "- Vision + Language (if multimodal)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š Tools & Frameworks\n",
    "\n",
    "- [ðŸ¤— HuggingFace Transformers](https://huggingface.co/transformers/)\n",
    "- [LangChain](https://www.langchain.com/)\n",
    "- [LlamaIndex](https://www.llamaindex.ai/)\n",
    "- [OpenAI API](https://platform.openai.com/)\n",
    "- [Mistral.ai](https://mistral.ai/)\n",
    "- [Open Source: Falcon, MosaicML, BLOOM, etc.]\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Challenges and Considerations\n",
    "\n",
    "- **Bias & Hallucination**: Can generate incorrect or offensive outputs\n",
    "- **Data Privacy**: Sensitive data risk during training or inference\n",
    "- **Cost & Resources**: High compute requirements\n",
    "- **Alignment & Safety**: Ensuring responses align with human values\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Reflection & Discussion\n",
    "\n",
    "> How do LLMs differ from traditional NLP models like RNNs and BERT?  \n",
    "> Can LLMs truly understand language, or are they simply mimicking patterns?\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Summary\n",
    "\n",
    "- LLMs are powerful, versatile generative models that have transformed NLP and AI applications.\n",
    "- Based on the transformer architecture, they learn from huge datasets and can generalize with minimal supervision.\n",
    "- Ongoing research continues to improve their efficiency, alignment, and multimodal capabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04d5dc0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
